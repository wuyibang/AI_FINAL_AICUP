{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import Libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport glob\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport matplotlib.pyplot as plt\nimport PIL\nimport random\nfrom PIL import Image\nimport json\n\n!pip install segmentation-models-pytorch --quiet\n!pip install -U albumentations --quiet\n\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset as BaseDataset\nimport torch.nn.functional as F\n\nimport torch\nimport numpy as np\nimport segmentation_models_pytorch as smp\nfrom torch import nn\n\nimport albumentations as albu","metadata":{"execution":{"iopub.status.busy":"2022-06-12T15:38:51.334685Z","iopub.execute_input":"2022-06-12T15:38:51.335384Z","iopub.status.idle":"2022-06-12T15:39:26.191297Z","shell.execute_reply.started":"2022-06-12T15:38:51.335347Z","shell.execute_reply":"2022-06-12T15:39:26.190441Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# from google.colab import drive\n# drive.mount('/content/drive')","metadata":{"id":"Yc65cmW3GcZE","outputId":"2f23d7d5-3ed5-44f4-d862-a660decfca22","execution":{"iopub.status.busy":"2022-06-12T15:39:26.193084Z","iopub.execute_input":"2022-06-12T15:39:26.193840Z","iopub.status.idle":"2022-06-12T15:39:26.198428Z","shell.execute_reply.started":"2022-06-12T15:39:26.193801Z","shell.execute_reply":"2022-06-12T15:39:26.197696Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## DataPath","metadata":{}},{"cell_type":"code","source":"# DATA_DIR_train = \"/content/drive/MyDrive/Colab Notebooks/AI_final_proj/Train_Images/\"  #training\n# DATA_DIR_mask = \"/content/drive/MyDrive/Colab Notebooks/AI_final_proj/Train_Masks/\" #training\n\n# DATA_DIR_train = \"AI_final_proj/Train_Images/\"  #training\n# DATA_DIR_mask = \"AI_final_proj/Train_Masks/\" #training\n\nDATA_DIR_train = \"../input/ai-final-dataset/AI_final_proj/Train_Images\"  \nDATA_DIR_mask = \"../input/ai-final-dataset/AI_final_proj/Train_Masks\" \n\n# DATA_DIR_vail = '/data/home/VGH_Seg_IMG_Label/Valid'\n# DATA_DIR_test = '/data/home/VGH_Seg_IMG_Label/Test'\n\n# SavePath = \"AI_final_proj/model_exp_final/\"\nSavePath = \"./\"\n\npath = SavePath\ncheckpoint = 'unet_effi_b1_50_normaug.pth'","metadata":{"id":"DmxZJf_ZGSaJ","execution":{"iopub.status.busy":"2022-06-12T15:39:26.199792Z","iopub.execute_input":"2022-06-12T15:39:26.200377Z","iopub.status.idle":"2022-06-12T15:39:26.209007Z","shell.execute_reply.started":"2022-06-12T15:39:26.200342Z","shell.execute_reply":"2022-06-12T15:39:26.208095Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Hyperparameters & Configs","metadata":{}},{"cell_type":"code","source":"RESIZE_SIZE = 32*20\nBATCH_SIZE = 8\nNUM_WORKERS = 2\n\nENCODER = 'efficientnet-b1'\nENCODER_WEIGHTS = 'imagenet'\nCLASSES = ['stas']\nACTIVATION = 'sigmoid' \nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nEPOCHS = 50\n\nSEED = 1187\n\nDEVICE","metadata":{"id":"7FZRB_z4-dYn","execution":{"iopub.status.busy":"2022-06-12T15:39:26.211275Z","iopub.execute_input":"2022-06-12T15:39:26.211900Z","iopub.status.idle":"2022-06-12T15:39:26.272014Z","shell.execute_reply.started":"2022-06-12T15:39:26.211823Z","shell.execute_reply":"2022-06-12T15:39:26.271103Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Seed Everything","metadata":{}},{"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\nseed_everything(SEED)","metadata":{"execution":{"iopub.status.busy":"2022-06-12T15:39:35.751941Z","iopub.execute_input":"2022-06-12T15:39:35.752306Z","iopub.status.idle":"2022-06-12T15:39:35.760084Z","shell.execute_reply.started":"2022-06-12T15:39:35.752273Z","shell.execute_reply":"2022-06-12T15:39:35.759162Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"### Get data & formatting","metadata":{}},{"cell_type":"code","source":"ids = os.listdir(DATA_DIR_train)\n# print(ids)\n# print(len(ids))","metadata":{"id":"QjO2uVgiXcPj","outputId":"35fe64a7-fc0d-4ef4-910d-7cb7d245e251","execution":{"iopub.status.busy":"2022-06-12T15:39:39.727132Z","iopub.execute_input":"2022-06-12T15:39:39.727524Z","iopub.status.idle":"2022-06-12T15:39:39.844451Z","shell.execute_reply.started":"2022-06-12T15:39:39.727491Z","shell.execute_reply":"2022-06-12T15:39:39.843653Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"images_list = [os.path.join(DATA_DIR_train, image_id) for image_id in ids]\n# display(images_list)\nprint(len(images_list))\nmasks_list = [os.path.join(DATA_DIR_mask, mask_id) for mask_id in ids]\n# display(masks_list)\nprint(len(masks_list))","metadata":{"id":"Q2ovGEoJXnhz","outputId":"ccfb79d6-43e3-4fed-d869-d26778d57d49","execution":{"iopub.status.busy":"2022-06-12T15:39:40.326195Z","iopub.execute_input":"2022-06-12T15:39:40.326984Z","iopub.status.idle":"2022-06-12T15:39:40.335611Z","shell.execute_reply.started":"2022-06-12T15:39:40.326946Z","shell.execute_reply":"2022-06-12T15:39:40.334891Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"for i in range(len(masks_list)):\n  masks_list[i] = masks_list[i].replace('.jpg','.png')\n# display(masks_list)","metadata":{"id":"5iQ0mertfjvU","execution":{"iopub.status.busy":"2022-06-12T15:39:40.747788Z","iopub.execute_input":"2022-06-12T15:39:40.748530Z","iopub.status.idle":"2022-06-12T15:39:40.754807Z","shell.execute_reply.started":"2022-06-12T15:39:40.748493Z","shell.execute_reply":"2022-06-12T15:39:40.753905Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### Splitting Data","metadata":{}},{"cell_type":"code","source":"\nx_train, x_valid, y_train, y_valid = train_test_split(images_list, masks_list, test_size=0.2, random_state=SEED)\nx_valid, x_test, y_valid, y_test = train_test_split(x_valid, y_valid, test_size=0.5, random_state=SEED)\n","metadata":{"id":"Hth89NVBXRvC","execution":{"iopub.status.busy":"2022-06-12T15:39:41.818904Z","iopub.execute_input":"2022-06-12T15:39:41.819532Z","iopub.status.idle":"2022-06-12T15:39:41.826386Z","shell.execute_reply.started":"2022-06-12T15:39:41.819499Z","shell.execute_reply":"2022-06-12T15:39:41.825567Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"print(len(x_train))\nprint(len(y_train))\nprint(len(x_valid))\nprint(len(y_valid))\nprint(len(x_test))\nprint(len(y_test))","metadata":{"id":"gKIzirtdY-j-","outputId":"68da6e38-28c1-4aa8-dc24-1800a09c555a","execution":{"iopub.status.busy":"2022-06-12T15:39:42.447964Z","iopub.execute_input":"2022-06-12T15:39:42.448622Z","iopub.status.idle":"2022-06-12T15:39:42.454037Z","shell.execute_reply.started":"2022-06-12T15:39:42.448585Z","shell.execute_reply":"2022-06-12T15:39:42.453061Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# y_train\n# # cv2.imread(x_train[0])\n# cv2.imread(y_train[0], 0)","metadata":{"id":"lqIhJvGRdIyt","outputId":"34205ff9-cfb2-42b2-a75d-b1684fad43af","execution":{"iopub.status.busy":"2022-06-12T15:39:46.548024Z","iopub.execute_input":"2022-06-12T15:39:46.548398Z","iopub.status.idle":"2022-06-12T15:39:46.552203Z","shell.execute_reply.started":"2022-06-12T15:39:46.548367Z","shell.execute_reply":"2022-06-12T15:39:46.551052Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### Data Visualization Func","metadata":{}},{"cell_type":"code","source":"# helper function for data visualization\ndef visualize(**images):\n    \"\"\"PLot images in one row.\"\"\"\n    n = len(images)\n    plt.figure(figsize=(16, 16))\n    for i, (name, image) in enumerate(images.items()):\n        plt.subplot(1, n, i + 1)\n        plt.xticks([])\n        plt.yticks([])\n        plt.title(' '.join(name.split('_')).title())\n        plt.imshow(image)\n    plt.show()","metadata":{"id":"hVXlJRMAGSaL","execution":{"iopub.status.busy":"2022-06-12T15:39:49.135104Z","iopub.execute_input":"2022-06-12T15:39:49.135807Z","iopub.status.idle":"2022-06-12T15:39:49.143620Z","shell.execute_reply.started":"2022-06-12T15:39:49.135766Z","shell.execute_reply":"2022-06-12T15:39:49.141879Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# from torch.utils.data import DataLoader\n# from torch.utils.data import Dataset as BaseDataset\n# import torch.nn.functional as F","metadata":{"id":"kmfzSgFaGSaL","execution":{"iopub.status.busy":"2022-06-12T15:39:50.716728Z","iopub.execute_input":"2022-06-12T15:39:50.717108Z","iopub.status.idle":"2022-06-12T15:39:50.720777Z","shell.execute_reply.started":"2022-06-12T15:39:50.717066Z","shell.execute_reply":"2022-06-12T15:39:50.720047Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Custom Dataset","metadata":{}},{"cell_type":"code","source":"\nclass Dataset(BaseDataset):\n    \"\"\"CamVid Dataset. Read images, apply augmentation and preprocessing transformations.\n    \n    Args:\n        images_dir (str): path to images folder\n        masks_dir (str): path to segmentation masks folder\n        class_values (list): values of classes to extract from segmentation mask\n        augmentation (albumentations.Compose): data transfromation pipeline \n            (e.g. flip, scale, etc.)\n        preprocessing (albumentations.Compose): data preprocessing \n            (e.g. noralization, shape manipulation, etc.)\n    \n    \"\"\"\n    \n    CLASSES = ['bg', 'stas']\n    \n    def __init__(\n            self, \n            images_list, \n            masks_list, \n            classes=None, \n            augmentation=None, \n            preprocessing=None,\n    ):\n        # self.ids = os.listdir(images_dir)\n        # self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ids]\n        # self.masks_fps = [os.path.join(masks_dir, image_id) for image_id in self.ids]\n        \n        self.images_fps = images_list\n        self.masks_fps = masks_list\n        \n        # convert str names to class values on masks\n        self.class_values = [self.CLASSES.index(cls.lower()) for cls in classes]\n        \n        self.augmentation = augmentation\n        self.preprocessing = preprocessing\n    \n    def __getitem__(self, i):\n        \n        # read data\n        # self.masks_fps[i] = self.masks_fps[i].replace('.jpg','.png')\n        image = cv2.imread(self.images_fps[i])\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        if image.shape != (RESIZE_SIZE, RESIZE_SIZE, 3):\n            image = cv2.resize(image, (RESIZE_SIZE, RESIZE_SIZE), interpolation=cv2.INTER_LANCZOS4)\n        \n        mask = cv2.imread(self.masks_fps[i], 0)\n        if mask.shape != (RESIZE_SIZE, RESIZE_SIZE, 3):\n            mask = cv2.resize(mask, (RESIZE_SIZE, RESIZE_SIZE), interpolation=cv2.INTER_LANCZOS4)  \n        mask = mask.astype('bool')\n        \n        # extract certain classes from mask (e.g. cars)\n        masks = [(mask == v) for v in self.class_values]\n        mask = np.stack(masks, axis=-1).astype('float')\n        \n        # apply augmentations\n        if self.augmentation:\n            sample = self.augmentation(image=image, mask=mask)\n            image, mask = sample['image'], sample['mask']\n        \n        # apply preprocessing\n        if self.preprocessing:\n            sample = self.preprocessing(image=image, mask=mask)\n            image, mask = sample['image'], sample['mask']\n            \n        return image, mask\n        \n    def __len__(self):\n        return len(self.images_fps)","metadata":{"id":"vwxR8gtrGSaM","execution":{"iopub.status.busy":"2022-06-12T15:39:51.778206Z","iopub.execute_input":"2022-06-12T15:39:51.778908Z","iopub.status.idle":"2022-06-12T15:39:51.791321Z","shell.execute_reply.started":"2022-06-12T15:39:51.778868Z","shell.execute_reply":"2022-06-12T15:39:51.790466Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Lets look at data we have\n\ndataset = Dataset(images_list=images_list, \n                  masks_list=masks_list, \n                  classes=['stas'])\n\nimage, mask = dataset[4] # get some sample\nvisualize(\n    image=image, \n    stas_mask=mask.squeeze(),\n)","metadata":{"id":"go4TAyr8GSaM","outputId":"22e3c934-902b-4d30-fbb7-3a6733936cab","execution":{"iopub.status.busy":"2022-06-12T15:39:52.354174Z","iopub.execute_input":"2022-06-12T15:39:52.354963Z","iopub.status.idle":"2022-06-12T15:39:52.805813Z","shell.execute_reply.started":"2022-06-12T15:39:52.354928Z","shell.execute_reply":"2022-06-12T15:39:52.805067Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"print(image.shape)\nprint(mask.shape)","metadata":{"id":"EAWUi1CrM8sh","outputId":"a4e79a49-0737-4699-e3fc-45dffa7f886d","execution":{"iopub.status.busy":"2022-06-12T15:39:53.018835Z","iopub.execute_input":"2022-06-12T15:39:53.019176Z","iopub.status.idle":"2022-06-12T15:39:53.024547Z","shell.execute_reply.started":"2022-06-12T15:39:53.019146Z","shell.execute_reply":"2022-06-12T15:39:53.023184Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"mask.squeeze().shape","metadata":{"id":"MiF8bH5gODUK","outputId":"9119cae3-a0e0-4382-af96-240ce201c71a","execution":{"iopub.status.busy":"2022-06-12T15:39:53.706340Z","iopub.execute_input":"2022-06-12T15:39:53.706966Z","iopub.status.idle":"2022-06-12T15:39:53.712294Z","shell.execute_reply.started":"2022-06-12T15:39:53.706933Z","shell.execute_reply":"2022-06-12T15:39:53.711551Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"## Data Augmentation","metadata":{}},{"cell_type":"code","source":"def get_training_augmentation():\n    train_transform = [\n\n        albu.HorizontalFlip(p=0.5),\n        \n        albu.VerticalFlip(p=0.5),\n        \n        albu.HueSaturationValue(p=0.6),\n        albu.Sharpen(p=0.5),\n        albu.RandomBrightnessContrast(p=0.4), \n        albu.Crop(x_min=0, y_min=0, x_max=RESIZE_SIZE, y_max=RESIZE_SIZE-40, p=0.5),\n        albu.PadIfNeeded(RESIZE_SIZE, RESIZE_SIZE),\n        \n        albu.OneOf(\n            [\n                albu.CLAHE(p=1),\n                albu.ToGray(p=0.3),\n                albu.RandomGamma(p=1),\n            ],\n            p=0.5,\n        ),   \n         albu.OneOf(\n            [\n                albu.Rotate(limit=30,p=1,border_mode=cv2.BORDER_CONSTANT),\n                albu.ShiftScaleRotate(scale_limit=0.5, rotate_limit=0, shift_limit=0.1, p=0.5, border_mode=cv2.BORDER_CONSTANT),\n            ],\n            p=0.8,\n             \n        ),\n        albu.Normalize(),\n        \n    ]\n    return albu.Compose(train_transform)\n\ndef to_tensor(x, **kwargs):\n    return x.transpose(2, 0, 1).astype('float32')\n","metadata":{"execution":{"iopub.status.busy":"2022-06-12T15:44:46.401434Z","iopub.execute_input":"2022-06-12T15:44:46.402046Z","iopub.status.idle":"2022-06-12T15:44:46.413174Z","shell.execute_reply.started":"2022-06-12T15:44:46.402012Z","shell.execute_reply":"2022-06-12T15:44:46.412348Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"\n\n# def get_training_augmentation():\n#     train_transform = [\n\n#         albu.HorizontalFlip(p=0.5),\n#         albu.Rotate(limit=40,p=1,border_mode=cv2.BORDER_CONSTANT),\n#         albu.VerticalFlip(p=0.5),\n#         albu.ShiftScaleRotate(scale_limit=0.5, rotate_limit=0, shift_limit=0.1, p=0.5, border_mode=cv2.BORDER_CONSTANT),\n        \n#         albu.HueSaturationValue(p=0.6),\n#         albu.Sharpen(p=0.5),\n#         albu.RandomBrightnessContrast(p=0.4),\n\n#         albu.Crop(x_min=0, y_min=0, x_max=RESIZE_SIZE, y_max=RESIZE_SIZE-40, p=0.5),\n#         albu.PadIfNeeded(RESIZE_SIZE, RESIZE_SIZE)\n\n        \n#     ]\n#     return albu.Compose(train_transform)\n\n# def to_tensor(x, **kwargs):\n#     return x.transpose(2, 0, 1).astype('float32')\n\n\ndef get_preprocessing(preprocessing_fn):\n    \"\"\"Construct preprocessing transform\n    \n    Args:\n        preprocessing_fn (callbale): data normalization function \n            (can be specific for each pretrained neural network)\n    Return:\n        transform: albumentations.Compose\n    \n    \"\"\"\n    \n    _transform = [\n        albu.Lambda(image=preprocessing_fn),\n        albu.Lambda(image=to_tensor, mask=to_tensor),\n    ]\n    return albu.Compose(_transform)","metadata":{"id":"ETSrDJK5GSaN","execution":{"iopub.status.busy":"2022-06-12T15:44:50.018363Z","iopub.execute_input":"2022-06-12T15:44:50.018709Z","iopub.status.idle":"2022-06-12T15:44:50.024499Z","shell.execute_reply.started":"2022-06-12T15:44:50.018680Z","shell.execute_reply":"2022-06-12T15:44:50.023504Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"### Visualize augmented imgs","metadata":{}},{"cell_type":"code","source":"#### Visualize resulted augmented images and masks\n\naugmented_dataset = Dataset(\n    images_list=images_list, \n    masks_list=masks_list, \n    augmentation=get_training_augmentation(), \n    classes=['stas'],\n)\n\n# same image with different random transforms\nfor i in range(3):\n    image, mask = augmented_dataset[1]\n    visualize(image=image, mask=mask.squeeze(-1))","metadata":{"id":"ew8uCoUsGSaO","outputId":"4e9e6e86-7baf-45a2-d967-bac9f09c9679","execution":{"iopub.status.busy":"2022-06-12T15:44:51.138615Z","iopub.execute_input":"2022-06-12T15:44:51.139001Z","iopub.status.idle":"2022-06-12T15:44:52.338751Z","shell.execute_reply.started":"2022-06-12T15:44:51.138969Z","shell.execute_reply":"2022-06-12T15:44:52.338057Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"## Create model and train","metadata":{"id":"xam15UbnGSaP"}},{"cell_type":"code","source":"\n# create segmentation model with pretrained encoder\nmodel = smp.Unet(\n    encoder_name=ENCODER, \n    encoder_weights=ENCODER_WEIGHTS, \n    classes=len(CLASSES), \n    activation=ACTIVATION,\n\n)\n\npreprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)","metadata":{"id":"1DyU1JV7GSaQ","execution":{"iopub.status.busy":"2022-06-12T15:45:01.596165Z","iopub.execute_input":"2022-06-12T15:45:01.596740Z","iopub.status.idle":"2022-06-12T15:45:03.732146Z","shell.execute_reply.started":"2022-06-12T15:45:01.596705Z","shell.execute_reply":"2022-06-12T15:45:03.731349Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"### DataLoader","metadata":{}},{"cell_type":"code","source":"train_dataset = Dataset(\n    x_train, \n    y_train, \n    augmentation=get_training_augmentation(), \n    preprocessing=get_preprocessing(preprocessing_fn),\n    classes=CLASSES,\n)\n\nvalid_dataset = Dataset(\n    x_valid, \n    y_valid, \n    preprocessing=get_preprocessing(preprocessing_fn),\n    classes=CLASSES,\n)\n\ntrain_loader = DataLoader(train_dataset, pin_memory=True, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\nvalid_loader = DataLoader(valid_dataset, pin_memory=True, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)","metadata":{"id":"G_KZ00CkGSaQ","execution":{"iopub.status.busy":"2022-06-12T15:45:06.313333Z","iopub.execute_input":"2022-06-12T15:45:06.313876Z","iopub.status.idle":"2022-06-12T15:45:06.319707Z","shell.execute_reply.started":"2022-06-12T15:45:06.313841Z","shell.execute_reply":"2022-06-12T15:45:06.318964Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# Dice/F1 score - https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient\n# IoU/Jaccard score - https://en.wikipedia.org/wiki/Jaccard_index\n\nloss = smp.utils.losses.DiceLoss()\nmetrics = [\n    smp.utils.metrics.Fscore(threshold=0.5),\n]\n\noptimizer = torch.optim.Adam([ \n    dict(params=model.parameters(), lr=0.0001 , betas=(0.9, 0.999), eps=1e-08, weight_decay=0.0001, amsgrad=False),\n])","metadata":{"id":"IBY7Wj8BGSaR","execution":{"iopub.status.busy":"2022-06-12T15:45:08.242648Z","iopub.execute_input":"2022-06-12T15:45:08.243192Z","iopub.status.idle":"2022-06-12T15:45:08.250646Z","shell.execute_reply.started":"2022-06-12T15:45:08.243156Z","shell.execute_reply":"2022-06-12T15:45:08.249822Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"# model","metadata":{"execution":{"iopub.status.busy":"2022-06-12T15:45:09.585909Z","iopub.execute_input":"2022-06-12T15:45:09.586297Z","iopub.status.idle":"2022-06-12T15:45:09.590510Z","shell.execute_reply.started":"2022-06-12T15:45:09.586263Z","shell.execute_reply":"2022-06-12T15:45:09.589451Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"# create epoch runners \n# it is a simple loop of iterating over dataloader`s samples\ntrain_epoch = smp.utils.train.TrainEpoch(\n    model, \n    loss=loss, \n    metrics=metrics, \n    optimizer=optimizer,\n    device=DEVICE,\n    verbose=True,\n)\n\nvalid_epoch = smp.utils.train.ValidEpoch(\n    model, \n    loss=loss, \n    metrics=metrics, \n    device=DEVICE,\n    verbose=True,\n)\n\n","metadata":{"id":"D4Pkhma8GSaR","execution":{"iopub.status.busy":"2022-06-12T15:45:10.336526Z","iopub.execute_input":"2022-06-12T15:45:10.337359Z","iopub.status.idle":"2022-06-12T15:45:14.948799Z","shell.execute_reply.started":"2022-06-12T15:45:10.337326Z","shell.execute_reply":"2022-06-12T15:45:14.947887Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"\n# os.makedirs(path)","metadata":{"id":"8jfpXDODGSaR","execution":{"iopub.status.busy":"2022-06-12T15:45:14.950516Z","iopub.execute_input":"2022-06-12T15:45:14.950847Z","iopub.status.idle":"2022-06-12T15:45:14.954203Z","shell.execute_reply.started":"2022-06-12T15:45:14.950813Z","shell.execute_reply":"2022-06-12T15:45:14.953500Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"import gc\ngc.collect()\ntorch.cuda.empty_cache()    ","metadata":{"id":"Md2bMRU9lHNL","execution":{"iopub.status.busy":"2022-06-12T15:45:14.955398Z","iopub.execute_input":"2022-06-12T15:45:14.955914Z","iopub.status.idle":"2022-06-12T15:45:15.186074Z","shell.execute_reply.started":"2022-06-12T15:45:14.955880Z","shell.execute_reply":"2022-06-12T15:45:15.185287Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"max_score = 0\n\ntrain_loss_list = []\ntrain_fscore_list = []\nvalid_loss_list = []\nvalid_fscore_list = []\n\nfor i in range(EPOCHS):\n    \n    print('\\nEpoch: {}'.format(i))\n    train_logs = train_epoch.run(train_loader)\n    valid_logs = valid_epoch.run(valid_loader)\n    train_loss_list.append(train_logs['dice_loss'])\n    train_fscore_list.append(train_logs['fscore'])\n    \n    valid_loss_list.append(valid_logs['dice_loss'])\n    valid_fscore_list.append(valid_logs['fscore'])\n    \n\n    if max_score < valid_logs['fscore']:\n        max_score = valid_logs['fscore']\n        torch.save(model, path+'/'+checkpoint)\n        print('Model saved!')\n\n        \n#     if i == 35:\n#         optimizer.param_groups[0]['lr'] = 1e-5\n#         print('Decrease decoder learning rate to 1e-5!')","metadata":{"id":"z67ThrorGSaR","outputId":"5d660243-cb78-4de6-c798-001b0c730818","scrolled":true,"execution":{"iopub.status.busy":"2022-06-12T15:45:16.440603Z","iopub.execute_input":"2022-06-12T15:45:16.441069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plot Result","metadata":{}},{"cell_type":"code","source":"plt.figure(0)\nplt.plot(train_fscore_list)\nplt.plot(valid_fscore_list)\nplt.title('Train Logs')\nplt.ylabel('accuracy')\nplt.xlabel('Epoch')\nplt.legend(['train', 'validation'], loc='lower right')\n\nplt.figure(1)\nplt.plot(train_loss_list)\nplt.plot(valid_loss_list)\nplt.title('Train Logs')\nplt.ylabel('loss')\nplt.xlabel('Epoch')\nplt.legend(['train', 'validation'], loc='upper right')\n\n\nplt.show()","metadata":{"id":"4G1V7gOrGSaS","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load  best model","metadata":{}},{"cell_type":"code","source":"# load best saved checkpoint\nbest_model = torch.load(SavePath +'/'+ checkpoint)","metadata":{"id":"CTyUB9VBGSaS","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# optimizer.param_groups[0]['lr']\nmax_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"code","source":"test_dataset = Dataset(\n    x_test, \n    y_test, \n    preprocessing=get_preprocessing(preprocessing_fn),\n    classes=CLASSES,\n)\n\ntest_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=NUM_WORKERS)","metadata":{"id":"vrFodpHGGSaS","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# evaluate model on test set\ntest_epoch = smp.utils.train.ValidEpoch(\n    model=best_model,\n    loss=loss,\n    metrics=metrics,\n    device=DEVICE,\n)\n\nlogs = test_epoch.run(test_dataloader)","metadata":{"id":"ZLxyGKjTGSaT","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test dataset without transformations for image visualization\ntest_dataset_vis = Dataset(\n    x_test, y_test, \n    classes=CLASSES,\n)","metadata":{"id":"I0d77jO5GSaT","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"out_path= SavePath + \"/\" + \"out_dir\"\n# os.makedirs(out_path)","metadata":{"id":"imoiaJemGSaT","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Compare Predictions with Ground Truth","metadata":{}},{"cell_type":"code","source":"\nfor i in range(len(test_dataset_vis)):\n    name = os.path.basename(test_dataset_vis.masks_fps[i])\n    print(name)\n    image_vis = test_dataset_vis[i][0].astype('uint8')\n    image, gt_mask = test_dataset[i]\n    x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n    pr_mask = best_model.predict(x_tensor)\n    pr_mask = (pr_mask.squeeze().cpu().numpy().round())\n    len(pr_mask)\n#     print(gt_mask.shape)\n    zeros = np.zeros((RESIZE_SIZE, RESIZE_SIZE, 3))\n    zeros[...,0] = pr_mask\n    zeros = cv2.resize(zeros, (1920, 828))\n#     print(i)\n    visualize( \n            image=image_vis, \n            ground_truth_mask=gt_mask.transpose(1, 2, 0)[...,0], \n            predicted_mask=pr_mask\n        )\n#     plt.imshow(zeros)\n#     plt.show()\n#     print(os.path.join(out_path, name.replace('.jpg','.png')))\n#     plt.imsave(os.path.join(out_path, name.split.replace('.jpg','.png'), zeros))\n    ","metadata":{"id":"86_tGMZiGSaT","scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"ycVTTqWlGSaU"},"execution_count":null,"outputs":[]}]}